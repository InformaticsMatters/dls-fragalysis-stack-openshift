---

# If there's a 'done' file in the de-duplication directory
# then we skip nextflow processing. This happens
# if we're re-running and do not want to 'clean-up'.

- name: Check de-duplciation directory.
  stat:
    path: "{{ dedupe_path }}/done"
  register: dedupe_done_file

- name: Check nextflow (entry)
  shell: echo -n $(pgrep java)
  register: nextflow_result
  when: not dedupe_done_file.stat.exists

- name: Get standard file
  shell: >-
    {{ scripts }}/graph_get_standard_file.py
    {{ process_path }}/standard-{{ process_standard_number }}
    {{ workflow_path }}
    --force
  environment:
    FRAGALYSIS_S3_BUCKET: "{{ s3_bucket }}"
  when:
  - not dedupe_done_file.stat.exists
  - nextflow_result.stdout|length == 0
  - nextflow_run|bool

- name: Copy nextflow files
  copy:
    src: "{{ item }}"
    dest: "{{ workflow_path }}"
  loop:
  - "graph-from-standard.nf"
  - "config"
  when:
  - not dedupe_done_file.stat.exists
  - nextflow_result.stdout|length == 0
  - nextflow_run|bool

- name: Set nextflow command
  set_fact:
    nextflow_cmd: >-
      nextflow -C config run graph-from-standard.nf
      --limit {{ process_limit }}
      --skip {{ process_skip }}
      --maxFrag {{ process_max_frag }}
      {{ nextflow_extra_args }}
      {{ nextflow_debug_args }}
  when:
  - not dedupe_done_file.stat.exists
  - nextflow_result.stdout|length == 0

- name: Display nextflow command
  debug:
    var: nextflow_cmd
  when:
  - not dedupe_done_file.stat.exists
  - nextflow_result.stdout|length == 0

- name: Run nextflow (async)
  command: "{{ nextflow_cmd }}"
  args:
    chdir: "{{ workflow_path }}"
  environment:
    NXF_PID_FILE: "{{ workflow_path }}/{{ nextflow_pid_file }}"
  async: "{{ nextflow_timeout_minutes|int * 60 }}"
  poll: 0
  register: nextflow_async
  when:
  - not dedupe_done_file.stat.exists
  - nextflow_result.stdout|length == 0
  - nextflow_run|bool

# Optionally wait for nextflow, polling at a user-defined interval...
# If the playbook is restarted the 'Wait' will not work,
# as it expects an 'ansible_job_id' which will have been lost.
# So we cannot run this if nextflow was found to be running

- name: Wait for nextflow
  async_status:
    jid: "{{ nextflow_async.ansible_job_id }}"
  register: nextflow_result
  until: nextflow_result.finished
  delay: "{{ nextflow_poll_period_minutes|int * 60 }}"
  retries: "{{ (nextflow_timeout_minutes|int / nextflow_poll_period_minutes|int)|int }}"
  when:
  - not dedupe_done_file.stat.exists
  - nextflow_result.stdout|length == 0
  - nextflow_run|bool

# Finally, just in case the playbook failed
# and was re-executed, check nextflow status again
# and set a fact for others.

- name: Check nextflow (exit)
  shell: echo -n $(pgrep java)
  register: nextflow_exit_result
  when: not dedupe_done_file.stat.exists

- name: Set nextflow 'done' fact
  set_fact:
    nextflow_done: "{{ (nextflow_exit_result.stdout|length == 0)|bool }}"
