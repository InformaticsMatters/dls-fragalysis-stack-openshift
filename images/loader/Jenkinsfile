#!groovyâ€‹

// The Fragalysis Stack (loader application) Builder.

pipeline {

  agent { label 'buildah-slave' }

  environment {
    // Registry details
    PROJECT = 'fragalysis-cicd'
    REGISTRY_USER = 'jenkins'
    REGISTRY = 'docker-registry.default:5000'
    REGISTRY_PRJ = "${REGISTRY}/${PROJECT}"
    STREAM_IMAGE = "${REGISTRY_PRJ}/loader-stream:latest"

    // get_unbuilt_data_directory environment...

    SOURCE_DATA_ROOT = '/fragalysis/django_data'
    TARGET_IMAGE = "${PROJECT}/loader-stream"
    INSIST_ON_READY = 'Yes'
    READY_FILE = 'READY'
    HOURLY_DATA = 'Yes'
    FORCE_BUILD = 'No'

    // Always push the image (whether it's new or not)
    ALWAYS_PUSH = 'No'

    // Slack channel for all notifications
    SLACK_BUILD_CHANNEL = 'dls-builds'
    // Slack channel to be used for errors/failures
    // (which also go to the CI/CD channel)
    SLACK_ALERT_CHANNEL = 'dls-alerts'

  }

  stages {

    stage('Inspect') {
      steps {
        dir('images') {
          // - Get the registry user API token (used in the Python module)
          // - Run the get_unbuilt_data_directory module to find the latest data
          // - Dump the script log.
          sh "ls ${SOURCE_DATA_ROOT}"
          slackSend channel: "#${SLACK_BUILD_CHANNEL}",
                    message: "${JOB_NAME} build ${BUILD_NUMBER} - starting..."
          script {

            env.CAUSE = 'UNKNOWN'
            def UpstreamCause = currentBuild.rawBuild.getCause(hudson.model.Cause$UpstreamCause)
            def UserCause = currentBuild.rawBuild.getCause(hudson.model.Cause$UserCause)
            def RemoteCause = currentBuild.rawBuild.getCause(hudson.model.Cause$RemoteCause)
            def TimerTriggerCause = currentBuild.rawBuild.getCause(hudson.triggers.TimerTrigger$TimerTriggerCause)
            def SCMTriggerCause = currentBuild.rawBuild.getCause(hudson.triggers.SCMTrigger$SCMTriggerCause)
            if (UpstreamCause) {
              env.CAUSE = 'UPSTREAM'
            } else if (UserCause) {
              env.CAUSE = 'USER'
            } else if (RemoteCause) {
              env.CAUSE = 'REMOTE'
            } else if (TimerTriggerCause) {
              env.CAUSE = 'TIMER'
            } else if (SCMTriggerCause) {
              env.CAUSE = 'SCM'
            }

            REGISTRY_USER_TOKEN = sh(script: 'oc whoami -t', returnStdout: true).trim()
            DATA_ORIGIN = sh(script: './get_unbuilt_data_directory.py', returnStdout: true).trim()

          }
          sh 'cat get_unbuilt_data_directory.log'
          sh "env"
        }
      }
    }

    stage('Build Image') {
      when {
        expression { DATA_ORIGIN.length() > 0 }
      }
      steps {
        dir('images/loader') {
          slackSend channel: "#${SLACK_BUILD_CHANNEL}",
                    message: "${JOB_NAME} build ${BUILD_NUMBER} - using '${DATA_ORIGIN}' data"
          sh "cp -r ${SOURCE_DATA_ROOT}/${DATA_ORIGIN}/* media"
          sh "buildah bud --format docker --build-arg DATA_ORIGIN=${DATA_ORIGIN} -f Dockerfile -t ${STREAM_IMAGE} ."
        }

      }
    }

    stage('Push Image') {
      when {
        expression { DATA_ORIGIN.length() > 0 || ALWAYS_PUSH == 'Yes' }
      }
      steps {
        withCredentials([string(credentialsId: 'clusterUrl', variable: 'CLUSTER_URL'),
                         string(credentialsId: 'ocUser', variable: 'OC_USER'),
                         string(credentialsId: 'ocUserPassword', variable: 'OC_USER_PASSWORD')]) {
          script {
            TOKEN = sh(script: 'oc whoami -t', returnStdout: true).trim()
          }
          sh "podman login --tls-verify=false --username ${REGISTRY_USER} --password ${TOKEN} ${REGISTRY}"
          sh "buildah push --tls-verify=false ${STREAM_IMAGE} docker://${STREAM_IMAGE}"
          sh "podman logout ${REGISTRY}"
          sh "buildah images"

          // With a new image pushed we need to launch the Loader 'Job'.
          // But we protect ourselves from doing this if a prior
          // instance of the 'Job' is running (which would be unusual).
          // To successfully launch the Job we first delete prior instances.
          // All of this takes place in the 'deploy.sh'
          sh "oc login ${CLUSTER_URL} -u ${OC_USER} -p ${OC_USER_PASSWORD}"
          sh "oc project ${PROJECT}"
          script {
            LOADER_RUNNING = sh(script: 'oc get jobs | grep fs-loader | grep Running | wc -l', returnStdout: true).trim()
          }
          echo "RUNNING=${LOADER_RUNNING}"
        }
      }
    }

    stage('Deploy Image') {
      when {
        expression { DATA_ORIGIN.length() > 0 && LOADER_RUNNING == '0' }
      }
      steps {
        dir('images/loader') {
          sh "oc delete job --selector template=fs-loader"
          sleep 4
          sh "oc process -f fs-loader.yaml | oc create -f -"
        }
      }
    }

    stage('Blocked Image') {
      when {
        expression { DATA_ORIGIN.length() > 0 && LOADER_RUNNING != '0' }
      }
      steps {
        echo "Loader is running (deployment is blocked)"
      }
      // Everything's OK but we were blocked from deploying it
      // as it appears a prior Job instance is running.
      // Here we just set the build status to UNSTABLE.
      //currentBuild.result = 'UNSTABLE'
    }

  }

  // Post-job actions.
  // See https://jenkins.io/doc/book/pipeline/syntax/#post
  post {

    success {
      slackSend channel: "#${SLACK_BUILD_CHANNEL}",
                color: 'good',
                message: "${JOB_NAME} build ${BUILD_NUMBER} - complete"
    }

    failure {
      slackSend channel: "#${SLACK_BUILD_CHANNEL}",
                color: 'danger',
                message: "${JOB_NAME} build ${BUILD_NUMBER} - failed (${BUILD_URL})"
      slackSend channel: "#${SLACK_ALERT_CHANNEL}",
                color: 'danger',
                message: "${JOB_NAME} build ${BUILD_NUMBER} - failed (${BUILD_URL})"
    }

    unstable {
      slackSend channel: "#${SLACK_BUILD_CHANNEL}",
                color: 'warning',
                message: "${JOB_NAME} build ${BUILD_NUMBER} - complete (unstable)"
    }

    fixed {
      slackSend channel: "#${SLACK_ALERT_CHANNEL}",
                color: 'good',
                message: "${JOB_NAME} build - fixed"
    }

  }

}
